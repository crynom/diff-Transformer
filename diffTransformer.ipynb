{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DiffAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(DiffAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size must be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.keys = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.queries = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, seq_length, _ = x.shape\n",
    "        values = self.values(x)\n",
    "        keys = self.keys(x)\n",
    "        queries = self.queries(x)\n",
    "\n",
    "        # Split into heads\n",
    "        values = values.view(N, seq_length, self.heads, self.head_dim)\n",
    "        keys = keys.view(N, seq_length, self.heads, self.head_dim)\n",
    "        queries = queries.view(N, seq_length, self.heads, self.head_dim)\n",
    "\n",
    "        values = values.permute(0, 2, 1, 3)  # (N, heads, seq_length, head_dim)\n",
    "        keys = keys.permute(0, 2, 1, 3)      # (N, heads, seq_length, head_dim)\n",
    "        queries = queries.permute(0, 2, 1, 3)  # (N, heads, seq_length, head_dim)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])  # (N, heads, seq_length, seq_length)\n",
    "        attention = F.softmax(energy, dim=3)\n",
    "\n",
    "        # Apply attention to values\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, seq_length, self.heads * self.head_dim)\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class DiffTransformer(nn.Module):\n",
    "    def __init__(self, embed_size, heads, num_layers, input_dim):\n",
    "        super(DiffTransformer, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attention = DiffAttention(embed_size, heads)\n",
    "        self.fc = nn.Linear(input_dim, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)  # Project input to embedding size\n",
    "        for _ in range(self.num_layers):\n",
    "            x = self.attention(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    embed_size = 256  # Embedding size\n",
    "    heads = 8         # Number of attention heads\n",
    "    num_layers = 6    # Number of transformer layers\n",
    "    input_dim = 128   # Input feature dimension\n",
    "    seq_length = 50   # Length of input sequences\n",
    "    batch_size = 32   # Batch size\n",
    "\n",
    "    model = DiffTransformer(embed_size, heads, num_layers, input_dim)\n",
    "    x = torch.rand(batch_size, seq_length, input_dim)  # Random input\n",
    "    output = model(x)\n",
    "    print(output.shape)  # Should output (batch_size, seq_length, embed_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Explanation\n",
    "- **DiffAttention Class**: Implements the differential attention mechanism. It computes attention scores using the dot product of queries and keys, applies softmax, and then uses these scores to weight the values.\n",
    "- **DiffTransformer Class**: Stacks multiple layers of the `DiffAttention` module. It projects the input features to the embedding size before passing them through the attention layers.\n",
    "- **Example Usage**: The example at the bottom demonstrates how to create an instance of the `DiffTransformer` and pass a random input through it.\n",
    "\n",
    "### Note\n",
    "This implementation is a basic version and may require further enhancements for specific tasks, such as adding positional encodings, layer normalization, or dropout for regularization. Adjust the parameters according to your dataset and task requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
